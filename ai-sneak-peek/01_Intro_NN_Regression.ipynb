{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"NN Regression.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"tCLjPJ8jt_-x"},"source":["# Your first venture into Deep Learning\n","\n","Hello, welcome to this last part of your journey towards deep learning and AI. Maybe it isn't the last part but really your first step.\n","So far you have learned a lot about Data Science, and the power of data and machine learning. Deep learning goes a step further and enables us to work with almost any kind of data, structured or unstructured, to gain insights and use from it. Deep learning is an important, probably the most important part for any \"weak\" AI System (that is a system that is only intelligent in a restricted setting) and for any upcoming strong or general AI.\n","\n","But before we build an AI Butler or a self driving car, we should start with something more familiar to you.\n","Deep learning essentially follows the same structure as any supervised machine learning: it can perform classification and regression given features and a target. (There are more advanced deep learning use cases like reinforcement learning, but we won't cover them here)\n","\n","Deep learning is a computer technique to extract and transform data–-with use cases ranging from human speech recognition to animal imagery classification–-by using multiple layers of neural networks. Each of these layers takes its inputs from previous layers and progressively refines them. The layers are trained by algorithms that minimize their errors and improve their accuracy. In this way, the network learns to perform a specified task.\n","\n","This first notebook will walk you through a regression task on tabular data (something you are familiar with) using deep learning.\n","A lot of people assume that you need all kinds of hard-to-understand stuff to get great results with deep learning, but as you'll see it can be quite simple.\n","We will be using the fast.ai library which is build on top of Pytorch and makes deep learning relatively simple. fast.ai also provides a course and a book, that make up a good portion of our Techlabs AI track, so if you haven't had enough after this track you know where to find more!\n","\n","You first need to have the following libraries installed.\n","If you find yourself without one of them, uncommend the corresponding line and install it (make sure this notebook is opened in the environment you want the package to be installed in)"]},{"cell_type":"code","metadata":{"id":"lRYY9d9tt_-2","executionInfo":{"status":"ok","timestamp":1611061758556,"user_tz":-60,"elapsed":1788,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}}},"source":["##not necessary if run in colabs:\n","#!pip install -q seaborn \n","##necessary even in colabs:\n","#!pip install -Uqq fastbook #if this does not work try:\n","#!pip install fastai==2.0\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from sklearn.metrics import mean_absolute_error\n","from fastai import * #good for experimentation not production code\n","from fastai.tabular.all import * #good for experimentation not production code"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YE6iGEuLt_-3"},"source":["# Regression using a deep neural net"]},{"cell_type":"markdown","metadata":{"id":"rFQNfG2Wt_-3"},"source":["**Remember:** In a regression problem, we aim to predict the output of a continuous value, like a price. Whereas in a classification problem, we aim to select a class from a list of classes (for example, where a picture contains a dog or a cat, recognizing which pet is in the image).\n","\n","This notebook uses the classic Auto MPG Dataset and builds a model to predict the fuel efficiency of late-1970s and early 1980s automobiles. \n","To do this, we'll provide the model with a description of many automobiles from that time period. \n","This description includes attributes like: cylinders, displacement, horsepower, and weight."]},{"cell_type":"markdown","metadata":{"id":"EHedWer3t_-4"},"source":["We load the data through the public url and name the columns accprding to the documentation. Run the code below to get a csv of the raw dataset."]},{"cell_type":"code","metadata":{"id":"EUg-EfMRt_-4","executionInfo":{"status":"ok","timestamp":1611061762545,"user_tz":-60,"elapsed":1424,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}}},"source":["url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n","column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n","                'Acceleration', 'Model Year', 'Origin']\n","\n","raw_dataset = pd.read_csv(url, names=column_names,\n","                          na_values='?', comment='\\t',\n","                          sep=' ', skipinitialspace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvgklOKKt_-4"},"source":["Let's look at the data to get a feel of what the dataset contains.\n","\n","#### NOW YOU\n","use functions and methods you know like `.head()` to get a good look at the raw data. Don't change anything in the data just yet."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"bqMqiSzot_-4","executionInfo":{"status":"ok","timestamp":1611061763931,"user_tz":-60,"elapsed":802,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"480c8639-45fd-48a9-f2b1-b423ef44c165"},"source":["data = raw_dataset.copy()\n","##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"wMriDaL3t_-5","executionInfo":{"status":"ok","timestamp":1611061765418,"user_tz":-60,"elapsed":746,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"b34c49d7-d511-42be-a16d-c1843da78281"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"id":"k4D_4q_bt_-6","executionInfo":{"status":"ok","timestamp":1611061766661,"user_tz":-60,"elapsed":1070,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"822e1db8-42ef-4dc1-cacc-1676e3ffc0a0"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RRToeDLJt_-6"},"source":["### Clean the data\n","As with any data application or ML algorithm Deep Learning requires relatively clean data. Sometimes the model itself may help you clean the data (e.g. in Image recognition) but more on that later. \n","\n","Let's check the data.\n","\n","The dataset may contain a few unknown or missing values.\n","#### NOW YOU \n","Find out how many missings are in each column of the dataset."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-tc3w2Mt_-6","executionInfo":{"status":"ok","timestamp":1611061768198,"user_tz":-60,"elapsed":710,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"31166f8f-a4a1-4f6d-c4b0-46fad7d66c5b"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTg_MwJqt_-6"},"source":["As there are only very few missing values we can just drop them."]},{"cell_type":"code","metadata":{"id":"Fa1w_dsot_-7","executionInfo":{"status":"ok","timestamp":1611061769463,"user_tz":-60,"elapsed":711,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}}},"source":["data.dropna(inplace = True) #careful with 'inplace = True', only use it if you are sure what the result is"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guSZF30ot_-7"},"source":["### Looking at the individual columns\n","\n","If you haven't inspected the individual columns yet, it is a good time to do so here."]},{"cell_type":"markdown","metadata":{"id":"bYgbvvG5t_-7"},"source":["If we look at the \"Origin\" column we see that it is categorical and not numeric. In order to make our neural network work we need to convert the categories to numbers and one-hot encode them.\n","\n","#### NOW YOU \n","\n","overwrite the 'Origin' column with the number mappins given in the origin_mappings dictionary."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"fi2ctBGNt_-7","executionInfo":{"status":"ok","timestamp":1611061770787,"user_tz":-60,"elapsed":731,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"68a23423-c999-4bfe-b8fb-4261658332ae"},"source":["origin_mappings = {1: 'USA', 2: 'Europe', 3: 'Japan'}\n","##YOUR CODE HERE\n","data.head()#check if everything is correct"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FLFbD6ljt_-7"},"source":["Next we will one-hot-encode our categories. There are multiple ways to do that, we will use plain pandas. Other ways would include scikit learn, and fast ai. Check out their documentation to find out how we might have accomplished this task through their functions. Maybe fast.ai would have been faster?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":215},"id":"hGEi34Evt_-8","executionInfo":{"status":"ok","timestamp":1611061772360,"user_tz":-60,"elapsed":888,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"88b5f19a-73b9-421e-d979-3b0cf386c194"},"source":["data = pd.get_dummies(data, prefix='', prefix_sep='')\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ys8fnUrBt_-8"},"source":["### Split the data into train and test\n","\n","#### NOW YOU\n","Split the dataset into a training set and a test set. Use 80% of the data for our training set. Set a random_state.\n","\n","We will use the test set in the final evaluation of our models and won't look at it before that to avoid any data."]},{"cell_type":"code","metadata":{"id":"NsD1Xdytt_-8","executionInfo":{"status":"ok","timestamp":1611061774624,"user_tz":-60,"elapsed":1014,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}}},"source":["##YOUR CODE HERE\n","##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e_mskHkdt_-8"},"source":["### Inspect the data visually\n","Let's have a quick look at the joint distribution of a few pairs of columns from the training set."]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":745},"id":"Ki-rEmfTt_-8","executionInfo":{"status":"ok","timestamp":1611061782194,"user_tz":-60,"elapsed":7256,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"5c759ec1-c470-41f2-fcf2-2777562707b1"},"source":["sns.pairplot(train[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X4G2QIOxt_-9"},"source":["#### NOW YOU\n","\n","Take your time and study the plot, try to spot some relationships. Think about which features could have the highest importance.\n","\n","\n","- ...\n","- ...\n","- ...\n","\n","\n","\n","\n","\n","**ONLY READ ON IF YOU HAVE CAREFULLY THOUGHT ABOUT THE TASK ABOVE**\n","\n","**No Cheating**\n","\n","\n","\n","Looking at the top row it should be clear that the fuel efficiency (MPG) is a function of all the other parameters. Looking at the other rows it should become evident that they are each functions of eachother."]},{"cell_type":"markdown","metadata":{"id":"fsffhIAQt_-9"},"source":["Let's also look at the overall statistics, note how each feature covers a very different range:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"1yeKLnWWt_-9","executionInfo":{"status":"ok","timestamp":1611061782195,"user_tz":-60,"elapsed":4234,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"13d11ee5-9801-4ff8-a26f-3bd955209426"},"source":["train.describe().T #.T transposes the output dataframe"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XmwHANftt_-9"},"source":["#### NOW YOU\n","Think about what this might mean for our Neural network, do we need to scale the features? Normalize them? Which range is appropriate and why? Note down your answer, we will get to this issue in a moment.\n","\n","- ...\n","- ...\n","- ...\n"]},{"cell_type":"markdown","metadata":{"id":"sMIPZEfht_--"},"source":["### Split features from labels\n","\n","We still need to separate the target value, the \"label\", from the features. This label is the value that you will train the model to predict. In our case 'MPG' (miles per gallon)"]},{"cell_type":"code","metadata":{"id":"bpY2dj7mt_--","executionInfo":{"status":"ok","timestamp":1611061782195,"user_tz":-60,"elapsed":2243,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}}},"source":["train_features = train.copy()\n","test_features = test.copy()\n","\n","train_labels = train_features.pop('MPG')\n","test_labels = test_features.pop('MPG')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qhwZ2RFnt_--"},"source":["#### BONUS: think about other ways of accomplishing the operation above and share them with your fellow learners on Slack"]},{"cell_type":"markdown","metadata":{"id":"klPa8Lzqt_--"},"source":["#### NOW YOU \n","Check if everything went as expected by looking at the data and shape of the data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"-OuFRfilt_--","executionInfo":{"status":"ok","timestamp":1611061783234,"user_tz":-60,"elapsed":1031,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"ac628f53-cc79-4ae6-84c4-cc7a55259d00"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tK8YCqqCt_--","executionInfo":{"status":"ok","timestamp":1611061784269,"user_tz":-60,"elapsed":1341,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"ca96679b-8157-48a8-abbd-d787b7240f1c"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_ZRuTrNt_-_","executionInfo":{"status":"ok","timestamp":1611061784573,"user_tz":-60,"elapsed":1085,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"d43b0d96-f07f-443d-c66d-01e7e0835524"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UcfuaApDt_-_","executionInfo":{"status":"ok","timestamp":1611061785226,"user_tz":-60,"elapsed":482,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"5b636a2d-7dd8-4188-def8-bb832bab0d24"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uy3fiSw_t__A"},"source":["### Normalization\n","Just a moment ago you thought about the different scales our features have. If you want to look again: In the table of statistics it's easy to see how different the ranges of each feature are."]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":318},"id":"gGY57ktDt__A","executionInfo":{"status":"ok","timestamp":1611061786820,"user_tz":-60,"elapsed":812,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"44d267d4-eba4-4424-ff1b-d755adecd7f5"},"source":["train_features.describe().T[[\"mean\", \"std\",\"min\",\"max\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wC1BblY1t__A"},"source":["It is good practice to normalize features that use different scales and ranges.\n","\n","One reason this is important is because in a Neural Network the features are multiplied by the model weights. So the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n","\n","Although a model might converge without feature normalization, normalization makes training much more stable and is a basic step in most Deep Learning workflows.\n","\n","\n","**fast.ai** makes Normalizing very easy. They provide a Dataloader which can load and transform your data as needed. The `from_df()` method takes the `procs=`argument which let's you specify which processing steps you want the DataLoader to perform. In our case we could have used `Categorify,Normalize` and maybe even others that handle missings. fast.ai provides a lot of these handy functionalities that you will learn about further down the road, or when you check out their extensive documentation.\n","\n","Here we will use the `TabularDataLoaders` which is a class that combines data loading and preparation. It makes our data 'ready' to be use by our deep learning model. If this seems a little 'black-box' right now it's fine. You can dig into the code and create more custom ways to load and prepare data later if you want. The AI-track will give you more knowledge on how to do these things. For now we will stick to the high level basics so that we can create something that works before going into all of the details.\n","\n","\n","\n","More about the tabular data loaders and related functions [here](https://docs.fast.ai/tabular.data.html)\n","\n","The `TabularDataLoaders` from fast.ai also expects you to pass the names of all continous and categorical variables.\n","\n","#### NOW YOU\n","List the categorical and continous variables from our dataframe below to pass them to the function."]},{"cell_type":"code","metadata":{"id":"2WMMuMT0t__A","executionInfo":{"status":"ok","timestamp":1611061798229,"user_tz":-60,"elapsed":733,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}}},"source":["cat_names = ##YOUR CODE HERE\n","cont_names = ##YOUR CODE HERE\n","procs = [Categorify,Normalize]\n","y_names = 'MPG'\n","dls = TabularDataLoaders.from_df(data, procs=procs, \n","                                 cat_names=##YOUR CODE HERE, \n","                                 cont_names=##YOUR CODE HERE, \n","                                 y_names= y_names, bs=64) #bs is the batch size more on that later\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V59qIz0pt__B"},"source":["We can use our dataloaders object (dls) and look at a batch with `.show_batch()`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"HPzsfFUBt__B","executionInfo":{"status":"ok","timestamp":1611061800180,"user_tz":-60,"elapsed":717,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"bf815d9a-9d6f-473a-a754-c33f44de5103"},"source":["dls.show_batch()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3gz4hzot__B"},"source":["## Training the model\n","\n","Now we need to specify the model. fast.ai makes this very easy, if you are just starting out you can use their functions and defaults to create working solutions without having to specify layers,activation functions, optimizers etc. (which is the goal here as we don't expect you to know everything from the start)\n","\n","We simply create a learner with `tabular_leaner()` pass in our dataloaders object and set a metric. In our case the mean-absolute-error will do fine, remember this is a regression problem."]},{"cell_type":"code","metadata":{"id":"waqd2n39t__B","executionInfo":{"status":"ok","timestamp":1611061838007,"user_tz":-60,"elapsed":723,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}}},"source":["learner = tabular_learner(dls, metrics = mae)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mslFl-CDt__B"},"source":["Now that we have specified the model it's time to train it.\n","fast.ai provides different ways to train a model, we will use the one cycle method. If you are curious what it does check out the documentation.\n","\n","In order to fit a model, we have to provide at least one piece of information: how many times to look at each data point (known as number of epochs). The number of epochs you select will largely depend on how much time you have available, and how long you find it takes in practice to fit your model. If you select a number that is too small, you can always train for more epochs later.\n","\n","Here we will use 35 epochs. If you want to learn how to select the number of epochs or what automatic methods for the epoch settings there are (e.g. early stopping) check out our AI track.\n","\n","### BONUS\n","Once you are done with this notebook, experiment with using more ore less epochs to train, notice what happens to the MAE and to the train and validation losses."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"WBb_mZ51t__C","executionInfo":{"status":"ok","timestamp":1611061929928,"user_tz":-60,"elapsed":4311,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"003e312d-a09f-41ea-d7a1-2505c06527c4"},"source":["learner.fit_one_cycle(35)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FHX0SoAbt__C"},"source":["## Model evaluation"]},{"cell_type":"markdown","metadata":{"id":"SGOnO-dpt__C"},"source":["After we have trained our model and looked at the validation MAE (that is the score on the validation set which was automatically created by fast.ai), it's time to see how it performs on unseen data. You can use the `learner.predict()` to get predictions for a single row of data. To get predictions on our test data (test_features) and compare the predicition to the actual labels  (test_labels) you can use the test_dl method from the DataLoaders. This will load the test dataframe and prepare it for predicitions. *Note*: this dataframe should not have the dependent variable in its columns, so we will just use our test_features not the test_labels."]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":308},"id":"RcKHBClGt__C","executionInfo":{"status":"ok","timestamp":1611062005390,"user_tz":-60,"elapsed":836,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"1436d339-e8ad-404e-fd64-2f3a1a6eba5b"},"source":["test_dl = learner.dls.test_dl(test_features)\n","\n","preds = learner.get_preds(dl = test_dl)\n","#preds is a tuple containing a tensor, extract tensor convert to array and unpack array for calculation of MAE\n","preds = np.array(preds[0].T)[0]\n","preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-B1nWWWwt__C"},"source":["#### NOW YOU \n","\n","Calculate the mean absolute error using the respective function from scikit-learn, the test labels and our predictions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PONPc3cmt__C","executionInfo":{"status":"ok","timestamp":1611062010719,"user_tz":-60,"elapsed":768,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"3e541ccf-19e0-45db-ac80-e4595058f79d"},"source":["##YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQOypPVnt__D"},"source":["If everything went correct this error looks really good. (in my version around 2.1, which is even slightly better than my validation error."]},{"cell_type":"markdown","metadata":{"id":"4LvRSDavt__D"},"source":["### Visualize predictions\n","\n","#### Scatterplot (with regression line)\n","\n","You can use this to visualize how close the predicitions are to the actual values. If everything worked well the dots should be very close to the diagonal line (which in this case would mean perfect prediction)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"G3dGU2tet__D","executionInfo":{"status":"ok","timestamp":1611062016570,"user_tz":-60,"elapsed":812,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"5909624e-91b3-48a3-c21a-f37a343f2b3e"},"source":["sns.regplot(x = test_labels, y = preds)\n","plt.xlabel('True Values [MPG]')\n","plt.ylabel('Predictions [MPG]')\n","sns.despine()\n","plt.xlim([5,45])\n","plt.ylim([5,45])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Mi_HIXJt__D"},"source":["#### Error distribution"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"pgElum9Gt__D","executionInfo":{"status":"ok","timestamp":1611062018952,"user_tz":-60,"elapsed":754,"user":{"displayName":"Merlin Schäfer","photoUrl":"https://lh5.googleusercontent.com/-qjKM6-nJ5vA/AAAAAAAAAAI/AAAAAAAAAEM/6QXLPvQFe1s/s64/photo.jpg","userId":"00421792487393303898"}},"outputId":"dd8b6e27-9c65-4b77-d2e1-94fa88919a5b"},"source":["error = preds - test_labels\n","sns.histplot(error, bins=25)\n","sns.despine()\n","plt.xlabel('Prediction Error [MPG]')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iP30DYzmt__D"},"source":["And that's basically it. Of course you can add steps, or use other techniques to gain more insights. But you have just trained a Neural Network on tabular data and gotten pretty good results.\n","\n","Check out the following notebooks to see how to use fast.ai and neural nets for image classification.\n","\n","### BONUS\n","\n","1. Compare your results with other methods (e.g linear regression, random forest, svm). Post your results in the slack channel.\n","\n","2. Try to reach the best MAE without overfitting. If necessary revise how to spot overfitting and discuss it with your fellow learners!"]},{"cell_type":"code","metadata":{"id":"XftfOsCIyFow"},"source":[],"execution_count":null,"outputs":[]}]}